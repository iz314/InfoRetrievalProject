{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4786a59b-07cd-4c3e-87af-458632e0e722",
   "metadata": {},
   "source": [
    "# Final Project - Earlier crawling version\n",
    "### EN.601.666 Information Retrieval and Web Agents\n",
    "### Isabel Dinan idinan1@jh.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fe8f5fef-9e32-4d73-9594-d44c305e606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdfplumber\n",
    "# !pip install nltk gensim matplotlib\n",
    "# !pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dd280a38-0641-4f66-9787-045318ab4072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "from io import BytesIO\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8fe8aa19-e2ab-4609-b6c5-4f75b8d7eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = os.path.join(os.getcwd(), \"pdfs\")\n",
    "FETCHED = []\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0af4c822-afc6-40fd-b7b8-163edcc8e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_paper_links(category, max_papers=50):\n",
    "    \"\"\"\n",
    "    Fetch links to paper abstracts from the arXiv category listing page.\n",
    "    \"\"\"\n",
    "    base_url = 'https://export.arxiv.org/list'\n",
    "    url = f\"{base_url}/{category}/recent\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    links = [f\"https://export.arxiv.org{a['href']}\" for a in soup.find_all('a', title='Abstract')][:max_papers]\n",
    "    \n",
    "    time.sleep(15)\n",
    "    \n",
    "    return links\n",
    "\n",
    "def fetch_paper_details(url):\n",
    "    \"\"\"\n",
    "    Fetch details of a paper given its abstract page URL.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.find('h1', class_='title').text.replace('Title: ', '').strip()\n",
    "    authors = ', '.join([author.text.strip() for author in soup.find_all('div', class_='authors')])\n",
    "    abstract = soup.find('blockquote', class_='abstract').text.replace('Abstract: ', '').strip()\n",
    "    meta_info = soup.find_all('div', class_='dateline')\n",
    "    submission_date = meta_info[0].text.strip() if meta_info else 'Not provided'\n",
    "    # submission_date = soup.find(text='Submitted').next.strip()\n",
    "    \n",
    "    pdf_link = url.replace('/abs/', '/pdf/') + '.pdf'\n",
    "    pdf_response = requests.get(pdf_link)\n",
    "    # if not os.path.exists(download_dir):\n",
    "    #     os.makedirs(download_dir)\n",
    "    # pdf_filename = os.path.join(download_dir, pdf_link.split('/')[-1])\n",
    "    # with open(pdf_filename, 'wb') as f:\n",
    "    #     f.write(pdf_response.content)\n",
    "    # print(f\"Downloaded PDF to {pdf_filename}\")\n",
    "    pdf_file = BytesIO(pdf_response.content)\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_file) as pdf:\n",
    "            pdf_text = ''.join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
    "        print(\"saved pdf for: \", pdf_link)\n",
    "    except Exception as e:\n",
    "        print(f\"PDFSyntaxError encountered for URL: {pdf_link}. Skipping this file.\")\n",
    "        pdf_text = \"\"\n",
    "    \n",
    "    time.sleep(15)\n",
    "    \n",
    "    return {'title': title, 'authors': authors, 'abstract': abstract, 'submission_date': submission_date, \n",
    "            'pdf_text': pdf_text, \n",
    "            'url': url}\n",
    "\n",
    "def crawl_arxiv(category, max_papers=50):\n",
    "    \"\"\"\n",
    "    Main function to crawl arXiv for papers in a specific category.\n",
    "    \"\"\"\n",
    "    links = fetch_paper_links(category, max_papers)\n",
    "    papers = []\n",
    "    for link in links:\n",
    "        if link not in FETCHED:\n",
    "            FETCHED.append(link)\n",
    "            paper = fetch_paper_details(link)\n",
    "            papers.append(paper)\n",
    "            print(f\"Fetched: {paper['title'][:30]}...\")\n",
    "    return papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ba4554ae-8cbf-4c61-8acb-b9f7f9a90dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'stat.ML'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c8be3435-5a5c-442c-9c00-551390856d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched: Title:\n",
      "Private Optimal Invento...\n",
      "PDFSyntaxError encountered for URL: https://export.arxiv.org/pdf/2404.15760.pdf. Skipping this file.\n",
      "Fetched: Title:\n",
      "Debiasing Machine Unlea...\n",
      "Fetched: Title:\n",
      "Long-term Off-Policy Ev...\n",
      "Fetched: Title:\n",
      "The Power of Resets in ...\n",
      "Fetched: Title:\n",
      "Insufficient Statistics...\n",
      "Fetched: Title:\n",
      "Hierarchical Hybrid Sli...\n",
      "Fetched: Title:\n",
      "Adversarial Robustness ...\n",
      "Fetched: Title:\n",
      "Edge-Efficient Deep Lea...\n",
      "Fetched: Title:\n",
      "Time topological analys...\n",
      "Fetched: Title:\n",
      "Maximum Discrepancy Gen...\n",
      "Fetched: Title:\n",
      "Estimating the Distribu...\n",
      "Fetched: Title:\n",
      "Gradient Guidance for D...\n",
      "Fetched: Title:\n",
      "Inference of Causal Net...\n",
      "Fetched: Title:\n",
      "Score matching for sub-...\n",
      "Fetched: Title:\n",
      "GIST: Gibbs self-tuning...\n",
      "Fetched: Title:\n",
      "PHLP: Sole Persistent H...\n",
      "Fetched: Title:\n",
      "Data-Driven Knowledge T...\n",
      "Fetched: Title:\n",
      "Conformal Predictive Sy...\n",
      "Fetched: Title:\n",
      "Variational Bayesian su...\n",
      "Fetched: Title:\n",
      "Second-order Informatio...\n",
      "Fetched: Title:\n",
      "Interpretable Predictio...\n",
      "Fetched: Title:\n",
      "A Bayesian Approach for...\n"
     ]
    }
   ],
   "source": [
    "ml_papers = crawl_arxiv(category, max_papers=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb5817-8291-48f6-a18e-711cd119e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df = pd.DataFrame(ml_papers)\n",
    "print(papers_df.head())  # Show the first few entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a8dd0-6b34-4cab-abd5-71addb073e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df.to_csv('machine_learning_papers_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1e507287-79dd-4edf-9810-233a7b28352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_stopwords():\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_stops = [\"http\", \"https\", \"et\", \"al\", \"url\", \"www\", \"com\", \"edu\", \n",
    "                    \"fig\", \"figure\", \"table\", \"pdf\", \"citation\", \"reference\", \n",
    "                    \"estimator\", \"method\", \"site\", \"meta\", \"using\", \"able\", \"never\",\n",
    "                    \"often\", \"mentioned\", \"others\", \"accordingly\", \"otherwise\",\n",
    "                    \"accross\", \"must\", \"much\", \"moreover\", \"might\", \"doi\", \"get\"]\n",
    "    stop_words.update(custom_stops)\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    stop_words = get_custom_stopwords()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def perform_topic_modeling(texts, num_topics=5):\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15, alpha='auto', eta='auto')\n",
    "    topics = lda_model.print_topics(num_words=5)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fe8ff55c-a101-4030-a82a-93d97d2cd535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: (0, '0.001*\"model\" + 0.001*\"distribution\" + 0.001*\"score\" + 0.001*\"nˆ\" + 0.001*\"aipw\"')\n",
      "Topic 2: (1, '0.001*\"model\" + 0.001*\"data\" + 0.001*\"score\" + 0.001*\"nˆ\" + 0.001*\"ipw\"')\n",
      "Topic 3: (2, '0.001*\"model\" + 0.001*\"nˆ\" + 0.001*\"effect\" + 0.001*\"proof\" + 0.001*\"aipw\"')\n",
      "Topic 4: (3, '0.001*\"model\" + 0.001*\"data\" + 0.001*\"nˆ\" + 0.001*\"aipw\" + 0.001*\"proof\"')\n",
      "Topic 5: (4, '0.017*\"model\" + 0.009*\"nˆ\" + 0.008*\"score\" + 0.008*\"data\" + 0.007*\"aipw\"')\n"
     ]
    }
   ],
   "source": [
    "papers_df['processed_text'] = papers_df['pdf_text'].apply(preprocess_text)\n",
    "topics = perform_topic_modeling(papers_df['processed_text'].tolist())\n",
    "for i, topic in enumerate(topics):\n",
    "    print(f\"Topic {i+1}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43315c69-da79-44e3-9ebb-ef32495aee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for text in papers_df['processed_text'] for word in text]\n",
    "word_freq = Counter(all_words)\n",
    "print(word_freq.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66496abd-e1e6-42b3-89cf-70b088c05fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
